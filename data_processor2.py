# -*- coding: utf-8 -*-
"""Untitled20.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HIQaiu0vcUKoPYQRIdm5BQ_jAgz8oBbc
"""

import pandas as pd  # Import pandas library for data manipulation
import ipaddress  # Import ipaddress library (not used in this code)
from sklearn.preprocessing import LabelEncoder  # Import LabelEncoder from scikit-learn for label encoding
import tensorflow as tf  # Import TensorFlow library
from sklearn.model_selection import train_test_split  # Import train_test_split function from scikit-learn

# Load the dataset from the CSV file
df = pd.read_csv("Darknet.csv")

# Drop unwanted columns from the dataset
df = df.drop(["Flow ID", "Timestamp", "Label2", "Src IP", "Dst IP"], axis=1)

# Remove rows with missing values
df = df.dropna()

# Create an instance of LabelEncoder and fit_transform the 'Label1' column
label_encoder1 = LabelEncoder()
df['Label1'] = label_encoder1.fit_transform(df['Label1'])

# Save the processed dataset to a new CSV file
df.to_csv("processed.csv", index=False)

# Load the processed dataset
df = pd.read_csv("processed.csv")

# Separate features and target variable
features = df.drop(['Label1'], axis=1)
label = df['Label1']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(features, label, test_size=0.2, random_state=42)

# Build the neural network model
model = tf.keras.Sequential([
    tf.keras.layers.Dense(250, activation='relu', input_shape=(features.shape[1],)),  # Input layer with 250 neurons and ReLU activation
    tf.keras.layers.Dense(128, activation='relu'),  # Hidden layer with 128 neurons and ReLU activation
    tf.keras.layers.Dense(64, activation='relu'),  # Hidden layer with 64 neurons and ReLU activation
    tf.keras.layers.Dense(4, activation='softmax')  # Output layer with 4 neurons and softmax activation (assuming 4 classes)
])

# Compile the model
model.compile(optimizer='adam',  # Use Adam optimizer
              loss='sparse_categorical_crossentropy',  # Use sparse categorical cross-entropy loss for multi-class classification
              metrics=['accuracy'])  # Track accuracy metric

# Train the model
model.fit(X_train, y_train, epochs=20, batch_size=512, validation_data=(X_test, y_test))  # Train for 20 epochs with batch size 512, using validation data

# Write test loss and accuracy to a text file
with open('accuracy2.txt', 'w') as f:
    f.write(f'Test Loss: {loss:.4f}\n')
    f.write(f'Test Accuracy: {accuracy:.4f}\n')